{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Configuration import printPeriodic,setDbgPrint,null\n",
    "from data_Generator import data_Generator,data_Generator_label\n",
    "import io, os, sys, types\n",
    "from Utils import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runTest():\n",
    "    def __init__(self ):\n",
    "        self.actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_features = 76\n",
    "        self.built_net()\n",
    "        self.TPcount = 0\n",
    "        self.FPcount = 0\n",
    "        self.TNcount =0\n",
    "        self.FNcount = 0\n",
    "        # sess = tf.InteractiveSession()\n",
    "        # self.saver.restore(sess, 'my_net/my_test_model.ckpt-20000')  # Load parameter\n",
    "        self.params = []\n",
    "        self.cost_his = []\n",
    "        self.time = 0\n",
    "        self.flag = 0\n",
    "        self.list = data_Generator(self.flag)\n",
    "        self.label = data_Generator_label(self.flag)\n",
    "        self.reward_history = []\n",
    "        self.count_history =[]\n",
    "        self.Pr_history = []\n",
    "        self.Rc_history = []\n",
    "        self.time_env_state = {'current': self.data()}\n",
    "        target_net = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        saver1 = tf.train.Saver(target_net)\n",
    "        self.sess = tf.Session()\n",
    "        #Upload model parameters\n",
    "        saver1.restore(self.sess, \"./my_net/target_net.ckpt-206000\")\n",
    "\n",
    "    def built_net(self):\n",
    "        tf.reset_default_graph()\n",
    "        # Reconstruction neural network model.Only one of the neural networks is kept here.\n",
    "       # c_names, n_l1, w_initializer, b_initializer = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
    "        n_l1 = 10\n",
    "        self.env_state_ = tf.placeholder(tf.float32, [None, self.n_features], name='env_state_')\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            with tf.variable_scope(\n",
    "                    'l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], collections=c_names)\n",
    "                l1 = tf.nn.tanh(tf.matmul(self.env_state_, w1) + b1)\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "\n",
    "    def choose_action(self, env_state):\n",
    "        env_state = env_state[np.newaxis, :]\n",
    "        actions_value = self.sess.run(self.q_next,feed_dict={self.env_state_: env_state})\n",
    "        action = np.argmax(actions_value)\n",
    "        return action\n",
    "\n",
    "    def data(self):\n",
    "        Network_data = self.list[self.time]\n",
    "        return Network_data\n",
    "\n",
    "    def update_State(self):\n",
    "        self.time_env_state[\"next\"] = self.data()\n",
    "        self.time += 1\n",
    "        return self.time_env_state[\"next\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.update_State()\n",
    "        return self.time_env_state[\"next\"]\n",
    "\n",
    "    def reward(self, action):\n",
    "\n",
    "        if action == self.label[self.time - 1][0]:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        value = 0\n",
    "        min_value = np.Inf\n",
    "        action_key = \"\"\n",
    "\n",
    "        #self.log('{}: action {} has min. value {}\\n'.format(self.time, action_key, min_value), period=self.statusPeriod,\n",
    "        #         counter=self.time)\n",
    "        # next_state = action\n",
    "        reward = self.reward(action)\n",
    "        self.reward_ = reward\n",
    "\n",
    "        self.count_history.append(self.reward_)\n",
    "        # self.correct_rate.append(self.count/(len(self.count_history)))\n",
    "\n",
    "        self.state = self.update_State()\n",
    "\n",
    "        return self.time_env_state[\"current\"], self.time_env_state[\"next\"], reward, self.label[self.time - 2][0]\n",
    "\n",
    "    def run_(self,cfg=None):\n",
    "        numEpisodes = cfg['numEpisodesTest']\n",
    "        maxSteps = cfg['maxStepsTest']\n",
    "        dbgPrint = cfg.get('dbgPrint', null)\n",
    "\n",
    "        statusPeriod = cfg.get('statusPeriod', 1)\n",
    "        for episode in range(1,numEpisodes+1):\n",
    "            # initial observation\n",
    "            step = 0\n",
    "            env_state_1 = self.reset()\n",
    "            while step < maxSteps:\n",
    "                #print('{}: current env = {}\\n'.format(self.time, self.time_env_state))\n",
    "                env_state = np.hstack(env_state_1)\n",
    "\n",
    "                action = self.choose_action(env_state)\n",
    "               # print('{}: action_ = {}, observation = {}\\n'.format(self.time, action_, observation))\n",
    "                env_state_, observation_,reward,label = self.step(action)\n",
    "                if label != 0 and action != 0:\n",
    "                    self.TPcount += 1\n",
    "                elif action != 0 and label == 0:\n",
    "                    self.FPcount += 1\n",
    "                elif action == 0 and label == 0:\n",
    "                    self.TNcount += 1\n",
    "                elif action == 0 and label != 0:\n",
    "                    self.FNcount += 1\n",
    "                if ((self.TPcount + self.FNcount) != 0 and (self.TPcount + self.FPcount) != 0):\n",
    "                    Pr = self.TPcount / (self.TPcount + self.FPcount)\n",
    "                    Rc = self.TPcount / (self.TPcount + self.FNcount)\n",
    "                    F1 = 2 / (1 / Pr + 1 / Rc)\n",
    "                    self.Rc_history.append(self.TPcount/(self.TPcount+self.FNcount))\n",
    "                    self.Pr_history.append(self.TPcount/(self.TPcount+self.FPcount))\n",
    "                    print('Rc = {},Pr = {},F1 = {}'.format(Rc,Pr,F1))\n",
    "\n",
    "                #print(\"env_state = {},\\nlabel={}, action = {},reward = {},\\nobservation_ ={}\\n\".format(env_state_, label,\n",
    "                 #                                                                                 action, reward,\n",
    "                  #                                                                                 observation_))\n",
    "                env_state_ = np.hstack(observation_)\n",
    "                env_state_1 = env_state_\n",
    "                step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
